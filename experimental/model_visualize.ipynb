{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from modules.unet import UNet_LS, UNet_LS_down, UNet_LS_up\n",
    "from transfers import UNetTransfer, Transfer, pretrained_transfers\n",
    "from logger import Logger, VisdomLogger\n",
    "from datasets import load_train_val, load_test, load_ood, load_all\n",
    "from task_configs import tasks, RealityTask\n",
    "from model_configs import model_types\n",
    "from models import DataParallelModel\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No deletion\n",
      "In (git) scaling-reset\n",
      "Logging to environment YKIVVA_jupyter_visualize_\n"
     ]
    }
   ],
   "source": [
    "PORT = 6932\n",
    "SERVER = \"10.90.47.7\"\n",
    "logger = VisdomLogger(\"Jupyter_vis\", env=\"YKIVVA_jupyter_visualize_\", port=PORT, server=SERVER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOWNLOAD MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = \"/scratch/kivva/projects/consistency_LS/models\"\n",
    "MODELS_MULTITASK = \"/scratch/kivva/projects/consistency_LS/multitask/results_mae/models\"\n",
    "MODELS_PERCEPTUAL_DIRECT = \"/scratch/kivva/projects/consistency_LS/consistency/results_PERCEPTUAL_x->n|r/models\"\n",
    "MODELS_PERCEPTUAL_BASELINE = \"/scratch/kivva/projects/consistency_LS/consistency/results_PERCEPTUAL_NORMAL_BASELINE/models\"\n",
    "\n",
    "rgb_down = model_types[\"rgb\"][\"down\"][0]()\n",
    "normal_down = model_types[\"normal\"][\"down\"][0]()\n",
    "depth_down = model_types[\"depth_zbuffer\"][\"down\"][0]()\n",
    "normal_up = model_types[\"normal\"][\"up\"][0]()\n",
    "depth_up = model_types[\"depth_zbuffer\"][\"up\"][0]()\n",
    "\n",
    "path_rgb_down = MODELS_PERCEPTUAL_DIRECT + '/rgb_down.pth'\n",
    "path_normal_down = MODELS_DIR + '/normal_down.pth'\n",
    "path_depth_down = MODELS_DIR + '/depth_zbuffer_down.pth'\n",
    "path_normal_up = MODELS_DIR + \"/normal_up.pth\"\n",
    "path_depth_up = MODELS_PERCEPTUAL_DIRECT + \"/depth_zbuffer_up.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(path_rgb_down):\n",
    "    rgb_down.load_weights(path_rgb_down)\n",
    "    print(\"1\")\n",
    "if os.path.isfile(path_normal_down):\n",
    "    normal_down.load_weights(path_normal_down)\n",
    "    print(\"2\")\n",
    "if os.path.isfile(path_depth_down):\n",
    "    depth_down.load_weights(path_depth_down)\n",
    "    print(\"3\")\n",
    "if os.path.isfile(path_normal_up):\n",
    "    normal_up.load_weights(path_normal_up)\n",
    "    print(\"4\")\n",
    "if os.path.isfile(path_depth_up):\n",
    "    depth_up.load_weights(path_depth_up)\n",
    "    print(\"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/consistency_shared/models/normal2zdepth_unet_v4.pth\n"
     ]
    }
   ],
   "source": [
    "rgb2depth, path = pretrained_transfers[('rgb', 'depth_zbuffer')]\n",
    "rgb2depth = DataParallelModel.load(rgb2depth(), path)\n",
    "\n",
    "rgb2normal, path = pretrained_transfers[('rgb', 'normal')]\n",
    "rgb2normal = DataParallelModel.load(rgb2normal(), path)\n",
    "\n",
    "normal2depth, path = pretrained_transfers[('normal', 'depth_zbuffer')]\n",
    "print(path)\n",
    "normal2depth = DataParallelModel.load(normal2depth(), path)\n",
    "\n",
    "depth2normal, path = pretrained_transfers[('depth_zbuffer', 'normal')]\n",
    "depth2normal = DataParallelModel.load(depth2normal(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb2normal_down = model_types[\"rgb\"][\"down\"][0]()\n",
    "rgb2depth_down = model_types[\"rgb\"][\"down\"][0]()\n",
    "rgb2normal_up = model_types[\"normal\"][\"up\"][0]()\n",
    "rgb2depth_up = model_types[\"depth_zbuffer\"][\"up\"][0]()\n",
    "\n",
    "path_rgb2normal_down = MODELS_DIR + '/rgb2normal_down.pth'\n",
    "path_rgb2depth_down = MODELS_DIR + '/rgb2depth_zbuffer_down.pth'\n",
    "path_rgb2normal_up = MODELS_DIR + \"/rgb2normal_up.pth\"\n",
    "path_rgb2depth_up = MODELS_DIR + \"/rgb2depth_zbuffer_up.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+\n",
      "+\n",
      "+\n",
      "+\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(path_rgb2normal_down):\n",
    "    rgb2normal_down.load_weights(path_rgb2normal_down)\n",
    "    print(\"+\")\n",
    "    \n",
    "if os.path.isfile(path_rgb2depth_down):\n",
    "    rgb2depth_down.load_weights(path_rgb2depth_down)\n",
    "    print(\"+\")\n",
    "\n",
    "if os.path.isfile(path_rgb2normal_up):\n",
    "    rgb2normal_up.load_weights(path_rgb2normal_up)\n",
    "    print(\"+\")\n",
    "    \n",
    "if os.path.isfile(path_rgb2depth_up):\n",
    "    rgb2depth_up.load_weights(path_rgb2depth_up)\n",
    "    print(\"+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images in almena:\n",
      "    rgb file len: 8785\n",
      "    Intersection files len:  8785\n",
      "number of images in albertville:\n",
      "    rgb file len: 7405\n",
      "    Intersection files len:  7405\n",
      "number of images in espanola:\n",
      "    rgb file len: 2282\n",
      "    Intersection files len:  2282\n"
     ]
    }
   ],
   "source": [
    "test_set = load_test(\n",
    "        [tasks.rgb, tasks.normal, tasks.depth_zbuffer],\n",
    "        buildings=['almena', 'albertville','espanola']\n",
    "    )\n",
    "test = RealityTask.from_static(\n",
    "        \"test\", test_set, [tasks.rgb, tasks.normal, tasks.depth_zbuffer]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_data = test.task_data[tasks.depth_zbuffer].to(DEVICE)\n",
    "normal_data = test.task_data[tasks.normal].to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    out = model_up(model_down(depth_data))\n",
    "    shape = list(normal_data.shape)\n",
    "    shape[1] = 3\n",
    "    out = out.clamp(min=0, max=1).expand(*shape)\n",
    "    depth_data = depth_data.clamp(min=0, max=1).expand(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [normal_data, out]\n",
    "\n",
    "logger.images_grouped(images, f\"multitask_ood:rgb->normal, rgb->depth\", resize=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    rgb file len: 8785\n",
      "    Intersection files len:  8785\n"
     ]
    }
   ],
   "source": [
    "tasks_set = [tasks.rgb, tasks.normal, tasks.depth_zbuffer]\n",
    "\n",
    "buildings = ['almena']\n",
    "batch_size = 64\n",
    "test_loader = load_all(tasks_set, buildings, batch_size=batch_size, split_file=\"../data/split.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCURACY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    depth_zbuffer file len: 8785\n",
      "    Intersection files len:  8785\n",
      "tensor(3.8416, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "src_task = tasks.depth_zbuffer\n",
    "target_task = tasks.normal\n",
    "tasks_set = [src_task, target_task]\n",
    "\n",
    "buildings = ['almena']\n",
    "batch_size = 64\n",
    "test_loader = load_all(tasks_set, buildings, batch_size=batch_size)\n",
    "\n",
    "test_iter = iter(test_loader)\n",
    "model_down = depth_down.to(DEVICE)\n",
    "model_up = normal_up.to(DEVICE)\n",
    "model = depth2normal.to(DEVICE)\n",
    "\n",
    "accuracy = 0\n",
    "for in_data, out_data in test_iter:\n",
    "    in_data = in_data.to(DEVICE)\n",
    "    out_data = out_data.to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "#         out = model(in_data)\n",
    "        out = model_up(model_down(in_data))\n",
    "    accuracy_tmp, _ = target_task.norm(out, out_data, compute_mse=False, batch_mean=False)\n",
    "    accuracy += accuracy_tmp.sum()\n",
    "\n",
    "print(accuracy / 8785. * 100.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERCEPTUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    rgb file len: 8785\n",
      "    Intersection files len:  8785\n",
      "tensor(9.3013, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "src_task = tasks.rgb\n",
    "middle_task = tasks.depth_zbuffer\n",
    "target_task = tasks.normal\n",
    "tasks_set = [src_task, target_task]\n",
    "\n",
    "buildings = ['almena']\n",
    "batch_size = 64\n",
    "test_loader = load_all(tasks_set, buildings, batch_size=batch_size)\n",
    "\n",
    "test_iter = iter(test_loader)\n",
    "\n",
    "model_down = rgb_down.to(DEVICE)\n",
    "model_up = depth_up.to(DEVICE)\n",
    "model_down_per = depth_down.to(DEVICE)\n",
    "model_up_per = normal_up.to(DEVICE)\n",
    "model_percep = depth2normal.to(DEVICE)\n",
    "\n",
    "accuracy = 0\n",
    "for in_data, out_data in test_iter:\n",
    "    in_data = in_data.to(DEVICE)\n",
    "    out_data = out_data.to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "#         out = model_percep(model_up(model_down(in_data)))\n",
    "        out = model_up_per(model_down_per(model_up(model_down(in_data))))\n",
    "    accuracy_tmp, _ = target_task.norm(out, out_data, compute_mse=False, batch_mean=False)\n",
    "    accuracy += accuracy_tmp.sum()\n",
    "\n",
    "print(accuracy / 8785. * 100.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTITASK APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTITASK_MODELS_DIR = \"/scratch/kivva/projects/consistency_LS/multitask/results_mae/models\"\n",
    "\n",
    "dest_tasks = [tasks.normal, tasks.depth_zbuffer]\n",
    "src_task = tasks.rgb\n",
    "\n",
    "model_rgb_down = model_types[\"rgb\"][\"down\"][0]()\n",
    "model_normal_up = model_types[\"normal\"][\"up\"][0]()\n",
    "model_depth_up = model_types[\"depth_zbuffer\"][\"up\"][0]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_rgb_down = MULTITASK_MODELS_DIR + '/rgb_down.pth'\n",
    "path_normal_up = MULTITASK_MODELS_DIR + \"/normal_up.pth\"\n",
    "path_depth_up = MULTITASK_MODELS_DIR + \"/depth_zbuffer_up.pth\"\n",
    "\n",
    "model_rgb_down.load_weights(path_rgb_down)\n",
    "model_normal_up.load_weights(path_normal_up)\n",
    "model_depth_up.load_weights(path_depth_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb2normal = UNet_LS(model_up=model_normal_up, model_down=model_rgb_down)\n",
    "rgb2depth = UNet_LS(model_up=model_depth_up, model_down=model_rgb_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ood images:  21\n"
     ]
    }
   ],
   "source": [
    "ood_set = load_ood([tasks.rgb])\n",
    "ood = RealityTask.from_static(\"ood\",  ood_set, [tasks.rgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No deletion\n",
      "In (git) scaling-reset\n",
      "Logging to environment visualize_models\n"
     ]
    }
   ],
   "source": [
    "logger = VisdomLogger(\"visualize\", env=\"visualize_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = ood.task_data[tasks.rgb].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb2normal.to(DEVICE)\n",
    "rgb2depth.to(DEVICE)\n",
    "rgb2normal = nn.DataParallel(rgb2normal) if not isinstance(rgb2normal, nn.DataParallel) else rgb2normal\n",
    "rgb2depth = nn.DataParallel(rgb2depth)if not isinstance(rgb2depth, nn.DataParallel) else rgb2depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input_data = input_data.to(DEVICE)\n",
    "    out_normal = rgb2normal(input_data)\n",
    "    input_data = input_data.to(DEVICE)\n",
    "    out_depth = rgb2depth(input_data)\n",
    "    \n",
    "    shape = list(out_normal.shape)\n",
    "    shape[1] = 3\n",
    "    out_normal = out_normal.clamp(min=0, max=1).expand(*shape)\n",
    "    out_depth = out_depth.clamp(min=0, max=1).expand(*shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [input_data, out_normal, out_depth]\n",
    "\n",
    "logger.images_grouped(images, f\"multitask_ood:rgb->normal, rgb->depth\", resize=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASELINE APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_MODELS_DIR = \"./baseline/models\"\n",
    "\n",
    "dest_tasks = [tasks.normal, tasks.depth_zbuffer]\n",
    "src_task = tasks.rgb\n",
    "\n",
    "baseline_rgb2normal, path_rgb2normal = pretrained_transfers[(src_task.name, dest_tasks[0].name)]\n",
    "baseline_rgb2depth, path_rgb2depth = pretrained_transfers[(src_task.name, dest_tasks[1].name)]\n",
    "path_rgb2normal = BASELINE_MODELS_DIR + \"/unet_baseline_standardval.pth\"\n",
    "path_rgb2depth = BASELINE_MODELS_DIR + \"/rgb2zdepth_buffer.pth\"\n",
    "baseline_rgb2depth = baseline_rgb2depth()\n",
    "baseline_rgb2normal = baseline_rgb2normal()\n",
    "baseline_rgb2depth = DataParallelModel(baseline_rgb2depth)\n",
    "baseline_rgb2normal = DataParallelModel(baseline_rgb2normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_rgb2depth.load_weights(path_rgb2depth)\n",
    "baseline_rgb2normal.load_weights(path_rgb2normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_rgb2depth.to(DEVICE)\n",
    "baseline_rgb2normal.to(DEVICE)\n",
    "input_data = ood.task_data[tasks.rgb].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out_normal = baseline_rgb2normal(input_data)\n",
    "    input_data.to(DEVICE)\n",
    "    out_depth = baseline_rgb2depth(input_data)\n",
    "    \n",
    "    shape = list(out_normal.shape)\n",
    "    shape[1] = 3\n",
    "    out_normal = out_normal.clamp(min=0, max=1).expand(*shape)\n",
    "    out_depth = out_depth.clamp(min=0, max=1).expand(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [input_data, out_normal, out_depth]\n",
    "\n",
    "logger.images_grouped(images, f\"baseline_ood:rgb->normal, rgb->depth\", resize=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZE ANOTHER MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = \"/home/kivva/Consistency_LD/multitask/results/results_Multitask:rgb->noraml|depth_zbuffer/models\"\n",
    "\n",
    "dest_tasks = [tasks.normal, tasks.depth_zbuffer]\n",
    "src_task = tasks.rgb\n",
    "\n",
    "model1_rgb_down = model_types[\"rgb\"][\"down\"][0]()\n",
    "model1_normal_up = model_types[\"normal\"][\"up\"][0]()\n",
    "model1_depth_up = model_types[\"depth_zbuffer\"][\"up\"][0]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_rgb_down = MULTITASK_MODELS_DIR + '/rgb_down.pth'\n",
    "path_normal_up = MULTITASK_MODELS_DIR + \"/normal_up.pth\"\n",
    "path_depth_up = MULTITASK_MODELS_DIR + \"/depth_zbuffer_up.pth\"\n",
    "\n",
    "model1_rgb_down.load_weights(path_rgb_down)\n",
    "model1_normal_up.load_weights(path_normal_up)\n",
    "model1_depth_up.load_weights(path_depth_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb2normal = UNet_LS(model_up=model1_normal_up, model_down=model1_rgb_down)\n",
    "rgb2depth = UNet_LS(model_up=model1_depth_up, model_down=model1_rgb_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = ood.task_data[tasks.rgb].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb2normal.to(DEVICE)\n",
    "rgb2depth.to(DEVICE)\n",
    "rgb2normal = nn.DataParallel(rgb2normal) if not isinstance(rgb2normal, nn.DataParallel) else rgb2normal\n",
    "rgb2depth = nn.DataParallel(rgb2depth)if not isinstance(rgb2depth, nn.DataParallel) else rgb2depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input_data = input_data.to(DEVICE)\n",
    "    out_normal = rgb2normal(input_data)\n",
    "    input_data = input_data.to(DEVICE)\n",
    "    out_depth = rgb2depth(input_data)\n",
    "    \n",
    "    shape = list(out_normal.shape)\n",
    "    shape[1] = 3\n",
    "    out_normal = out_normal.clamp(min=0, max=1).expand(*shape)\n",
    "    out_depth = out_depth.clamp(min=0, max=1).expand(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [input_data, out_normal, out_depth]\n",
    "\n",
    "logger.images_grouped(images, f\"model_ood:rgb->normal, rgb->depth\", resize=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
