{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from modules.unet import UNet_LS, UNet_LS_down, UNet_LS_up\n",
    "from transfers import UNetTransfer, Transfer, pretrained_transfers\n",
    "from logger import Logger, VisdomLogger\n",
    "from datasets import load_train_val, load_test, load_ood\n",
    "from task_configs import tasks, RealityTask\n",
    "from model_configs import model_types\n",
    "from models import DataParallelModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTITASK APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTITASK_MODELS_DIR = \"/scratch/kivva/projects/consistency_LS/multitask/results_mae/models\"\n",
    "\n",
    "dest_tasks = [tasks.normal, tasks.depth_zbuffer]\n",
    "src_task = tasks.rgb\n",
    "\n",
    "model_rgb_down = model_types[\"rgb\"][\"down\"][0]()\n",
    "model_normal_up = model_types[\"normal\"][\"up\"][0]()\n",
    "model_depth_up = model_types[\"depth_zbuffer\"][\"up\"][0]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_rgb_down = MULTITASK_MODELS_DIR + '/rgb_down.pth'\n",
    "path_normal_up = MULTITASK_MODELS_DIR + \"/normal_up.pth\"\n",
    "path_depth_up = MULTITASK_MODELS_DIR + \"/depth_zbuffer_up.pth\"\n",
    "\n",
    "model_rgb_down.load_weights(path_rgb_down)\n",
    "model_normal_up.load_weights(path_normal_up)\n",
    "model_depth_up.load_weights(path_depth_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb2normal = UNet_LS(model_up=model_normal_up, model_down=model_rgb_down)\n",
    "rgb2depth = UNet_LS(model_up=model_depth_up, model_down=model_rgb_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ood images:  21\n"
     ]
    }
   ],
   "source": [
    "ood_set = load_ood([tasks.rgb])\n",
    "ood = RealityTask.from_static(\"ood\",  ood_set, [tasks.rgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No deletion\n",
      "In (git) scaling-reset\n",
      "Logging to environment visualize_models\n"
     ]
    }
   ],
   "source": [
    "logger = VisdomLogger(\"visualize\", env=\"visualize_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = ood.task_data[tasks.rgb].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb2normal.to(DEVICE)\n",
    "rgb2depth.to(DEVICE)\n",
    "rgb2normal = nn.DataParallel(rgb2normal) if not isinstance(rgb2normal, nn.DataParallel) else rgb2normal\n",
    "rgb2depth = nn.DataParallel(rgb2depth)if not isinstance(rgb2depth, nn.DataParallel) else rgb2depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input_data = input_data.to(DEVICE)\n",
    "    out_normal = rgb2normal(input_data)\n",
    "    input_data = input_data.to(DEVICE)\n",
    "    out_depth = rgb2depth(input_data)\n",
    "    \n",
    "    shape = list(out_normal.shape)\n",
    "    shape[1] = 3\n",
    "    out_normal = out_normal.clamp(min=0, max=1).expand(*shape)\n",
    "    out_depth = out_depth.clamp(min=0, max=1).expand(*shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [input_data, out_normal, out_depth]\n",
    "\n",
    "logger.images_grouped(images, f\"multitask_ood:rgb->normal, rgb->depth\", resize=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASELINE APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_MODELS_DIR = \"./baseline/models\"\n",
    "\n",
    "dest_tasks = [tasks.normal, tasks.depth_zbuffer]\n",
    "src_task = tasks.rgb\n",
    "\n",
    "baseline_rgb2normal, path_rgb2normal = pretrained_transfers[(src_task.name, dest_tasks[0].name)]\n",
    "baseline_rgb2depth, path_rgb2depth = pretrained_transfers[(src_task.name, dest_tasks[1].name)]\n",
    "path_rgb2normal = BASELINE_MODELS_DIR + \"/unet_baseline_standardval.pth\"\n",
    "path_rgb2depth = BASELINE_MODELS_DIR + \"/rgb2zdepth_buffer.pth\"\n",
    "baseline_rgb2depth = baseline_rgb2depth()\n",
    "baseline_rgb2normal = baseline_rgb2normal()\n",
    "baseline_rgb2depth = DataParallelModel(baseline_rgb2depth)\n",
    "baseline_rgb2normal = DataParallelModel(baseline_rgb2normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_rgb2depth.load_weights(path_rgb2depth)\n",
    "baseline_rgb2normal.load_weights(path_rgb2normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_rgb2depth.to(DEVICE)\n",
    "baseline_rgb2normal.to(DEVICE)\n",
    "input_data = ood.task_data[tasks.rgb].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out_normal = baseline_rgb2normal(input_data)\n",
    "    input_data.to(DEVICE)\n",
    "    out_depth = baseline_rgb2depth(input_data)\n",
    "    \n",
    "    shape = list(out_normal.shape)\n",
    "    shape[1] = 3\n",
    "    out_normal = out_normal.clamp(min=0, max=1).expand(*shape)\n",
    "    out_depth = out_depth.clamp(min=0, max=1).expand(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [input_data, out_normal, out_depth]\n",
    "\n",
    "logger.images_grouped(images, f\"baseline_ood:rgb->normal, rgb->depth\", resize=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZE ANOTHER MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = \"/home/kivva/Consistency_LD/multitask/results/results_Multitask:rgb->noraml|depth_zbuffer/models\"\n",
    "\n",
    "dest_tasks = [tasks.normal, tasks.depth_zbuffer]\n",
    "src_task = tasks.rgb\n",
    "\n",
    "model1_rgb_down = model_types[\"rgb\"][\"down\"][0]()\n",
    "model1_normal_up = model_types[\"normal\"][\"up\"][0]()\n",
    "model1_depth_up = model_types[\"depth_zbuffer\"][\"up\"][0]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_rgb_down = MULTITASK_MODELS_DIR + '/rgb_down.pth'\n",
    "path_normal_up = MULTITASK_MODELS_DIR + \"/normal_up.pth\"\n",
    "path_depth_up = MULTITASK_MODELS_DIR + \"/depth_zbuffer_up.pth\"\n",
    "\n",
    "model1_rgb_down.load_weights(path_rgb_down)\n",
    "model1_normal_up.load_weights(path_normal_up)\n",
    "model1_depth_up.load_weights(path_depth_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb2normal = UNet_LS(model_up=model1_normal_up, model_down=model1_rgb_down)\n",
    "rgb2depth = UNet_LS(model_up=model1_depth_up, model_down=model1_rgb_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = ood.task_data[tasks.rgb].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb2normal.to(DEVICE)\n",
    "rgb2depth.to(DEVICE)\n",
    "rgb2normal = nn.DataParallel(rgb2normal) if not isinstance(rgb2normal, nn.DataParallel) else rgb2normal\n",
    "rgb2depth = nn.DataParallel(rgb2depth)if not isinstance(rgb2depth, nn.DataParallel) else rgb2depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input_data = input_data.to(DEVICE)\n",
    "    out_normal = rgb2normal(input_data)\n",
    "    input_data = input_data.to(DEVICE)\n",
    "    out_depth = rgb2depth(input_data)\n",
    "    \n",
    "    shape = list(out_normal.shape)\n",
    "    shape[1] = 3\n",
    "    out_normal = out_normal.clamp(min=0, max=1).expand(*shape)\n",
    "    out_depth = out_depth.clamp(min=0, max=1).expand(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [input_data, out_normal, out_depth]\n",
    "\n",
    "logger.images_grouped(images, f\"model_ood:rgb->normal, rgb->depth\", resize=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
